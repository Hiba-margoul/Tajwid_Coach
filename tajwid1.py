import re
import unicodedata
import json
from typing import List, Dict, Set

class EnhancedTajweedAnalyzer:
    """Analyseur de Tajwid amÃ©liorÃ© avec rÃ¨gles Ã©tendues"""
    
    def __init__(self):
        self.character_categories = self._initialize_character_categories()
        self.rules = self._initialize_enhanced_rules()
        self.normalization_cache = {}
    
    def _initialize_character_categories(self) -> Dict[str, str]:
        """CatÃ©gories de caractÃ¨res Ã©tendues"""
        return {
            'solar_letters': "ØªØ«Ø¯Ø°Ø±Ø²Ø³Ø´ØµØ¶Ø·Ø¸Ù„Ù†",
            'lunar_letters': "Ø§Ø¨Ø¬Ø­Ø®Ø¹ØºÙÙ‚ÙƒÙ„Ù…Ù†Ù‡ÙˆÙŠØ¡",
            'huroof_halaq': "Ø¡Ù‡Ø¹Ø­ØºØ®",
            'huroof_idgham_ghunnah': "ÙŠÙˆÙ…Ù†",
            'huroof_idgham_bila_ghunnah': "Ø±Ù„",
            'huroof_iqlab': "Ø¨",
            'huroof_ikhfa': "ØªØ«Ø¬Ø¯ÙÙ‚ÙƒØ·Ø¸Ø¶ØµØ´Ø³Ø°Ø²",
            'huroof_qalqala': "Ù‚Ø·Ø¨Ø¬Ø¯",
            'huroof_madd': "Ø§ÙˆÙŠ",
            'huroof_ghunnah': "Ù†Ù…"
        }
    
    def _initialize_enhanced_rules(self) -> Dict[str, List[str]]:
        """RÃ¨gles Ã©tendues avec patterns amÃ©liorÃ©s"""
        cats = self.character_categories
        
        return {
            # Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ù†ÙˆÙ† Ø§Ù„Ø³Ø§ÙƒÙ†Ø© ÙˆØ§Ù„ØªÙ†ÙˆÙŠÙ† - Ù…Ø­Ø³Ù†Ø©
            'izhar_hulqi': [
                r'Ù†[Ù’ÙŒÙÙ‹][Ø¡Ù‡Ø¹Ø­ØºØ®]',
                r'[Ù‹ÙÙŒ][Ø¡Ù‡Ø¹Ø­ØºØ®]',
                r'Ù†[Û¡][Ø¡Ù‡Ø¹Ø­ØºØ®]'
            ],
            'idgham_ghunnah': [
                r'Ù†[Ù’ÙŒÙÙ‹][ÙŠÙˆÙ…Ù†]',
                r'[Ù‹ÙÙŒ][ÙŠÙˆÙ…Ù†]',
                r'Ù†[Û¡][ÙŠÙˆÙ…Ù†]'
            ],
            'idgham_bila_ghunnah': [
                r'Ù†[Ù’ÙŒÙÙ‹][Ø±Ù„]',
                r'[Ù‹ÙÙŒ][Ø±Ù„]',
                r'Ù†[Û¡][Ø±Ù„]'
            ],
            'iqlab': [
                r'Ù†[Ù’ÙŒÙÙ‹]Ø¨',
                r'[Ù‹ÙÙŒ]Ø¨',
                r'Ù†[Û¡]Ø¨',
                r'Ù†Û¢Ø¨'
            ],
            'ikhfa': [
                r'Ù†[Ù’ÙŒÙÙ‹][' + cats['huroof_ikhfa'] + ']',
                r'[Ù‹ÙÙŒ][' + cats['huroof_ikhfa'] + ']',
                r'Ù†[Û¡][' + cats['huroof_ikhfa'] + ']'
            ],
            
            # Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ù…ÙŠÙ… Ø§Ù„Ø³Ø§ÙƒÙ†Ø© - Ù…Ø­Ø³Ù†Ø©
            'ikhfa_shafawi': [
                r'Ù…[Ù’]Ø¨',
                r'Ù…[Û¡]Ø¨'
            ],
            'idgham_mithlayn': [
                r'Ù…[Ù’]Ù…',
                r'Ù…[Û¡]Ù…'
            ],
            'izhar_shafawi': [
                r'Ù…[Ù’][^Ù…Ø¨]',
                r'Ù…[Û¡][^Ù…Ø¨]'
            ],
            
            # Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ù…Ø¯ÙˆØ¯ - Ù…ÙˆØ³Ø¹Ø©
            'madd_tabii': [
                r'[Ø§ÙˆÙŠ][^Ù’Ù‘Û¡\s]',
                r'[Ø§ÙˆÙŠÙ°][^Ù’Ù‘Û¡\s]'
            ],
            'madd_muttasil': [
                r'[Ø§ÙˆÙŠ]Ø¡\w',
                r'[Ø§ÙˆÙŠÙ°]Ø¡\w',
                r'[Ø§ÙˆÙŠ][Ù“]Ø¡'
            ],
            'madd_munfasil': [
                r'[Ø§ÙˆÙŠ]\s+[Ø£Ø¥Ø¤Ø¦Ø¡]',
                r'[Ø§ÙˆÙŠÙ°]\s+[Ø£Ø¥Ø¤Ø¦Ø¡]'
            ],
            'madd_lazim': [
                r'[Ø§ÙˆÙŠ][Ù‘]',
                r'[Ø§ÙˆÙŠÙ°][Ù‘]',
                r'[Ø§ÙˆÙŠ][Ù’]\w',
                r'[Ø§ÙˆÙŠÙ°][Ù’]\w'
            ],
            'madd_arid': [
                r'[Ø§ÙˆÙŠ][Ù’]\s',
                r'[Ø§ÙˆÙŠÙ°][Ù’]\s'
            ],
            
            # Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ø±Ø§Ø¡ - Ù…Ø­Ø³Ù†Ø©
            'tafkhim_ra': [
                r'Ø±[ÙÙ]',
                r'Ø±[Ù’Û¡][^Ù]',
                r'Ø±[ÙÙ‘][^Ù]'
            ],
            'tarqiq_ra': [
                r'Ø±[Ù]',
                r'Ø±[Ù’Û¡][Ù]',
                r'Ø±[ÙÙ‘][Ù]'
            ],
            
            # Ø§Ù„Ù‚Ù„Ù‚Ù„Ø© - Ù…ÙˆØ³Ø¹Ø©
            'qalqala': [
                r'[Ù‚Ø·Ø¨Ø¬Ø¯][Ù’]',
                r'[Ù‚Ø·Ø¨Ø¬Ø¯][Û¡]',
                r'[Ù‚Ø·Ø¨Ø¬Ø¯][ÙÙ‘]'
            ],
            
            # Ø§Ù„ØºÙ†Ø© - Ù…ÙˆØ³Ø¹Ø©
            'ghunnah': [
                r'[Ù†Ù…][Ù‘]',
                r'[Ù†Ù…][ÙÙ‘]',
                r'Ù†[Ù’][ÙŠÙˆÙ…Ù†Ø¨]',
                r'Ù†[Û¡][ÙŠÙˆÙ…Ù†Ø¨]'
            ],
            
            # Ø£Ø­ÙƒØ§Ù… Ø®Ø§ØµØ© Ø¨Ø§Ù„Ù‡Ù…Ø²Ø§Øª
            'hamzat_wasl': [
                r'Ù±\w+',
                r'Ø§(?:Ø³Øª|Ù†|Ù…|Øª|Ù|Ø³|ÙŠ|Ø§)\w+'
            ],
            'hamzat_qat': [
                r'[Ø£Ø¥][^\s]'
            ]
        }
    
    def _normalize(self, text: str) -> str:
        """Normalisation amÃ©liorÃ©e"""
        if text in self.normalization_cache:
            return self.normalization_cache[text]
        
        # Remplacer les caractÃ¨res spÃ©ciaux du Coran
        replacements = {
            'Ûª': '', 'Û¥': '', 'Û–': '', 'Û—': '', 'Û˜': '', 'Û™': '', 'Ûš': '', 'Û›': '', 'Ûœ': '',
            'Ù': '', 'Ù°': 'Ø§', 'Û¦': '', 'Û­': '', 'Û«': '', 'Û¬': '', 'Û©': '', 'Û¨': '', 'Û§': '',
            'Û ': '', 'Û¡': 'Ù’', 'Û¢': '', 'Û£': '', 'Û¤': '', 'Û¥': '', 'Û¦': '', 'Û§': '', 'Û¨': '',
            'Û©': '', 'Ûª': '', 'Û«': '', 'Û¬': '', 'Û­': '', 'Û®': '', 'Û¯': ''
        }
        
        normalized = unicodedata.normalize("NFC", text)
        for old, new in replacements.items():
            normalized = normalized.replace(old, new)
        
        self.normalization_cache[text] = normalized
        return normalized
    
    def get_rule_explanation(self, rule_name: str) -> str:
        """Explications Ã©tendues des rÃ¨gles"""
        explanations = {
            'izhar_hulqi': 'Ø§Ù„Ø¥Ø¸Ù‡Ø§Ø± Ø§Ù„Ø­Ù„Ù‚ÙŠ: Ø¥Ø¸Ù‡Ø§Ø± Ø§Ù„Ù†ÙˆÙ† Ø§Ù„Ø³Ø§ÙƒÙ†Ø© Ø£Ùˆ Ø§Ù„ØªÙ†ÙˆÙŠÙ† Ø¹Ù†Ø¯ Ø­Ø±ÙˆÙ Ø§Ù„Ø­Ù„Ù‚ (Ø¡ØŒ Ù‡ØŒ Ø¹ØŒ Ø­ØŒ ØºØŒ Ø®)',
            'idgham_ghunnah': 'Ø§Ù„Ø¥Ø¯ØºØ§Ù… Ø¨ØºÙ†Ø©: Ø¥Ø¯ØºØ§Ù… Ø§Ù„Ù†ÙˆÙ† Ø§Ù„Ø³Ø§ÙƒÙ†Ø© Ø£Ùˆ Ø§Ù„ØªÙ†ÙˆÙŠÙ† ÙÙŠ Ø§Ù„ÙŠØ§Ø¡ØŒ Ø§Ù„ÙˆØ§ÙˆØŒ Ø§Ù„Ù…ÙŠÙ…ØŒ Ø§Ù„Ù†ÙˆÙ† Ù…Ø¹ Ø§Ù„ØºÙ†Ø©',
            'idgham_bila_ghunnah': 'Ø§Ù„Ø¥Ø¯ØºØ§Ù… Ø¨Ù„Ø§ ØºÙ†Ø©: Ø¥Ø¯ØºØ§Ù… Ø§Ù„Ù†ÙˆÙ† Ø§Ù„Ø³Ø§ÙƒÙ†Ø© Ø£Ùˆ Ø§Ù„ØªÙ†ÙˆÙŠÙ† ÙÙŠ Ø§Ù„Ø±Ø§Ø¡ØŒ Ø§Ù„Ù„Ø§Ù… Ø¨Ø¯ÙˆÙ† ØºÙ†Ø©',
            'iqlab': 'Ø§Ù„Ø¥Ù‚Ù„Ø§Ø¨: Ù‚Ù„Ø¨ Ø§Ù„Ù†ÙˆÙ† Ø§Ù„Ø³Ø§ÙƒÙ†Ø© Ø£Ùˆ Ø§Ù„ØªÙ†ÙˆÙŠÙ† Ù…ÙŠÙ…Ø§Ù‹ Ù…Ø¹ Ø§Ù„ØºÙ†Ø© Ø¹Ù†Ø¯ Ø§Ù„Ø¨Ø§Ø¡',
            'ikhfa': 'Ø§Ù„Ø¥Ø®ÙØ§Ø¡: Ø¥Ø®ÙØ§Ø¡ Ø§Ù„Ù†ÙˆÙ† Ø§Ù„Ø³Ø§ÙƒÙ†Ø© Ø£Ùˆ Ø§Ù„ØªÙ†ÙˆÙŠÙ† Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø¨Ø§Ù‚ÙŠØ© Ù…Ø¹ Ø§Ù„ØºÙ†Ø©',
            'ikhfa_shafawi': 'Ø§Ù„Ø¥Ø®ÙØ§Ø¡ Ø§Ù„Ø´ÙÙˆÙŠ: Ø¥Ø®ÙØ§Ø¡ Ø§Ù„Ù…ÙŠÙ… Ø§Ù„Ø³Ø§ÙƒÙ†Ø© Ø¹Ù†Ø¯ Ø§Ù„Ø¨Ø§Ø¡ Ù…Ø¹ Ø§Ù„ØºÙ†Ø©',
            'idgham_mithlayn': 'Ø¥Ø¯ØºØ§Ù… Ø§Ù„Ù…Ø«Ù„ÙŠÙ† Ø§Ù„ØµØºÙŠØ±: Ø¥Ø¯ØºØ§Ù… Ø§Ù„Ù…ÙŠÙ… Ø§Ù„Ø³Ø§ÙƒÙ†Ø© ÙÙŠ Ø§Ù„Ù…ÙŠÙ…',
            'izhar_shafawi': 'Ø§Ù„Ø¥Ø¸Ù‡Ø§Ø± Ø§Ù„Ø´ÙÙˆÙŠ: Ø¥Ø¸Ù‡Ø§Ø± Ø§Ù„Ù…ÙŠÙ… Ø§Ù„Ø³Ø§ÙƒÙ†Ø© Ø¹Ù†Ø¯ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø­Ø±ÙˆÙ Ø¹Ø¯Ø§ Ø§Ù„Ø¨Ø§Ø¡ ÙˆØ§Ù„Ù…ÙŠÙ…',
            'madd_tabii': 'Ø§Ù„Ù…Ø¯ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ: Ù…Ø¯ Ø­Ø±ÙˆÙ Ø§Ù„Ù…Ø¯ (Ø§ØŒ ÙˆØŒ ÙŠ) Ø¨Ù…Ù‚Ø¯Ø§Ø± Ø­Ø±ÙƒØªÙŠÙ†',
            'madd_muttasil': 'Ø§Ù„Ù…Ø¯ Ø§Ù„Ù…ØªØµÙ„: ÙˆØ¬ÙˆØ¨ Ø§Ù„Ù…Ø¯ Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙ„ØªÙ‚ÙŠ Ø­Ø±Ù Ø§Ù„Ù…Ø¯ Ù…Ø¹ Ø§Ù„Ù‡Ù…Ø²Ø© ÙÙŠ ÙƒÙ„Ù…Ø© ÙˆØ§Ø­Ø¯Ø©',
            'madd_munfasil': 'Ø§Ù„Ù…Ø¯ Ø§Ù„Ù…Ù†ÙØµÙ„: Ø¬ÙˆØ§Ø² Ø§Ù„Ù…Ø¯ Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙ„ØªÙ‚ÙŠ Ø­Ø±Ù Ø§Ù„Ù…Ø¯ Ù…Ø¹ Ø§Ù„Ù‡Ù…Ø²Ø© ÙÙŠ ÙƒÙ„Ù…ØªÙŠÙ†',
            'madd_lazim': 'Ø§Ù„Ù…Ø¯ Ø§Ù„Ù„Ø§Ø²Ù…: ÙˆØ¬ÙˆØ¨ Ø§Ù„Ù…Ø¯ Ø¨Ù…Ù‚Ø¯Ø§Ø± Ø³Øª Ø­Ø±ÙƒØ§Øª Ø¹Ù†Ø¯ ÙˆØ¬ÙˆØ¯ Ø³ÙƒÙˆÙ† Ø£ØµÙ„ÙŠ',
            'madd_arid': 'Ø§Ù„Ù…Ø¯ Ø§Ù„Ø¹Ø§Ø±Ø¶ Ù„Ù„Ø³ÙƒÙˆÙ†: Ø¬ÙˆØ§Ø² Ø§Ù„Ù…Ø¯ Ø¹Ù†Ø¯ Ø§Ù„ÙˆÙ‚Ù Ø¹Ù„Ù‰ Ø­Ø±Ù Ø§Ù„Ù…Ø¯',
            'tafkhim_ra': 'ØªÙØ®ÙŠÙ… Ø§Ù„Ø±Ø§Ø¡: ØªØºÙ„ÙŠØ¸ ØµÙˆØª Ø§Ù„Ø±Ø§Ø¡ Ø¹Ù†Ø¯ Ø§Ù„ÙØªØ­ Ø£Ùˆ Ø§Ù„Ø¶Ù… Ø£Ùˆ Ø§Ù„Ø³ÙƒÙˆÙ† Ø¨Ø¹Ø¯ ÙØªØ­ Ø£Ùˆ Ø¶Ù…',
            'tarqiq_ra': 'ØªØ±Ù‚ÙŠÙ‚ Ø§Ù„Ø±Ø§Ø¡: ØªØ±Ù‚ÙŠÙ‚ ØµÙˆØª Ø§Ù„Ø±Ø§Ø¡ Ø¹Ù†Ø¯ Ø§Ù„ÙƒØ³Ø± Ø£Ùˆ Ø§Ù„Ø³ÙƒÙˆÙ† Ø¨Ø¹Ø¯ ÙƒØ³Ø±',
            'qalqala': 'Ø§Ù„Ù‚Ù„Ù‚Ù„Ø©: Ø§Ù‡ØªØ²Ø§Ø² Ø§Ù„ØµÙˆØª Ø¹Ù†Ø¯ Ø§Ù„Ù†Ø·Ù‚ Ø¨Ø­Ø±Ù Ø³Ø§ÙƒÙ† Ù…Ù† Ø­Ø±ÙˆÙ Ù‚Ø·Ø¨ Ø¬Ø¯',
            'ghunnah': 'Ø§Ù„ØºÙ†Ø©: ØµÙˆØª ÙŠØ®Ø±Ø¬ Ù…Ù† Ø§Ù„Ø®ÙŠØ´ÙˆÙ… ÙÙŠ Ø§Ù„Ù†ÙˆÙ† ÙˆØ§Ù„Ù…ÙŠÙ… Ø§Ù„Ù…Ø´Ø¯Ø¯ØªÙŠÙ† ÙˆÙÙŠ Ø§Ù„Ø¥Ø¯ØºØ§Ù… ÙˆØ§Ù„Ø¥Ù‚Ù„Ø§Ø¨',
            'hamzat_wasl': 'Ù‡Ù…Ø²Ø© Ø§Ù„ÙˆØµÙ„: ØªØ³Ù‚Ø· Ø¹Ù†Ø¯ Ø§Ù„ÙˆØµÙ„ ÙˆØªØ«Ø¨Øª Ø¹Ù†Ø¯ Ø§Ù„Ø§Ø¨ØªØ¯Ø§Ø¡',
            'hamzat_qat': 'Ù‡Ù…Ø²Ø© Ø§Ù„Ù‚Ø·Ø¹: ØªØ«Ø¨Øª ÙˆØµÙ„Ø§Ù‹ ÙˆØ§Ø¨ØªØ¯Ø§Ø¡'
        }
        return explanations.get(rule_name, "Ø´Ø±Ø­ ØºÙŠØ± Ù…ØªÙˆÙØ±")
    
    def _check_lam_rules(self, text: str, position: int) -> List[Dict]:
        """VÃ©rification amÃ©liorÃ©e des rÃ¨gles de Lam"""
        rules_found = []
        
        # VÃ©rifier Lam Shamsiyya/Qamariya
        if position > 0 and text[position] == 'Ù„':
            # VÃ©rifier le contexte "Ø§Ù„"
            if position >= 2 and text[position-2:position] == "Ø§Ù„":
                if position + 1 < len(text):
                    next_char = text[position+1]
                    if next_char in self.character_categories['solar_letters']:
                        rules_found.append({
                            'rule': 'lam_shamsiyya',
                            'explanation': 'Ù„Ø§Ù… Ø´Ù…Ø³ÙŠØ©: Ø¥Ø¯ØºØ§Ù… Ø§Ù„Ù„Ø§Ù… Ø§Ù„Ø³Ø§ÙƒÙ†Ø© ÙÙŠ Ø§Ù„Ø­Ø±Ù Ø§Ù„Ø´Ù…Ø³ÙŠ Ø§Ù„ØªØ§Ù„ÙŠ',
                            'context': text[max(0, position-2):min(len(text), position+3)]
                        })
                    elif next_char in self.character_categories['lunar_letters']:
                        rules_found.append({
                            'rule': 'lam_qamariyya',
                            'explanation': 'Ù„Ø§Ù… Ù‚Ù…Ø±ÙŠØ©: Ø¥Ø¸Ù‡Ø§Ø± Ø§Ù„Ù„Ø§Ù… Ø§Ù„Ø³Ø§ÙƒÙ†Ø© Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ù‚Ù…Ø±ÙŠØ©',
                            'context': text[max(0, position-2):min(len(text), position+3)]
                        })
        
        return rules_found
    
    def _check_special_cases(self, text: str, position: int) -> List[Dict]:
        """VÃ©rification des cas spÃ©ciaux"""
        rules_found = []
        char = text[position]
        context = text[max(0, position-2):min(len(text), position+3)]
        
        # VÃ©rifier les cas spÃ©ciaux de Madd
        if char in self.character_categories['huroof_madd']:
            # VÃ©rifier Madd Munfasil (entre deux mots)
            if position > 0 and text[position-1].isspace():
                if position + 1 < len(text) and text[position+1] in ['Ø£', 'Ø¥', 'Ø¡']:
                    rules_found.append({
                        'rule': 'madd_munfasil',
                        'explanation': self.get_rule_explanation('madd_munfasil'),
                        'context': context
                    })
        
        # VÃ©rifier les cas de Ghunnah supplÃ©mentaires
        if char in self.character_categories['huroof_ghunnah']:
            if position + 1 < len(text) and text[position+1] in ['Ù‘', 'ÙÙ‘']:
                rules_found.append({
                    'rule': 'ghunnah',
                    'explanation': self.get_rule_explanation('ghunnah'),
                    'context': context
                })
        
        return rules_found
    
    def analyze_character(self, text: str, position: int) -> Dict:
        """Analyse de caractÃ¨re amÃ©liorÃ©e"""
        char = text[position]
        context = text[max(0, position-2):min(len(text), position+3)]
        
        detected_rules = []
        used_rules = set()
        
        # VÃ©rifier toutes les rÃ¨gles standards
        for rule_name, pattern_list in self.rules.items():
            if rule_name in used_rules:
                continue
                
            for pattern in pattern_list:
                # VÃ©rifier Ã  partir de la position actuelle
                text_to_check = text[position:]
                match = re.search(pattern, text_to_check)
                
                if match and match.start() == 0:
                    detected_rules.append({
                        'rule': rule_name,
                        'explanation': self.get_rule_explanation(rule_name),
                        'context': context
                    })
                    used_rules.add(rule_name)
                    break
        
        # VÃ©rifier les rÃ¨gles spÃ©ciales de Lam
        lam_rules = self._check_lam_rules(text, position)
        for lam_rule in lam_rules:
            if lam_rule['rule'] not in used_rules:
                detected_rules.append(lam_rule)
                used_rules.add(lam_rule['rule'])
        
        # VÃ©rifier les cas spÃ©ciaux
        special_rules = self._check_special_cases(text, position)
        for special_rule in special_rules:
            if special_rule['rule'] not in used_rules:
                detected_rules.append(special_rule)
                used_rules.add(special_rule['rule'])
        
        return {
            'position': position,
            'character': char,
            'rules': detected_rules,
            'total_rules': len(detected_rules),
            'context': context
        }
    
    def analyze_verse(self, verse: str) -> Dict:
        """Analyse de verset amÃ©liorÃ©e"""
        verse = self._normalize(verse)
        analysis_results = []
        
        i = 0
        while i < len(verse):
            if verse[i].isspace():
                i += 1
                continue
                
            char_analysis = self.analyze_character(verse, i)
            analysis_results.append(char_analysis)
            i += 1
        
        return {
            'verse': verse,
            'analysis': analysis_results,
            'statistics': self._generate_statistics(analysis_results),
            'summary': self._generate_summary(analysis_results)
        }
    
    def _generate_statistics(self, analysis_results: List[Dict]) -> Dict:
        """GÃ©nÃ©ration de statistiques amÃ©liorÃ©e"""
        total_rules = sum(result['total_rules'] for result in analysis_results)
        total_chars = len(analysis_results)
        
        rules_by_type = {}
        for result in analysis_results:
            for rule in result['rules']:
                rule_name = rule['rule']
                rules_by_type[rule_name] = rules_by_type.get(rule_name, 0) + 1
        
        return {
            'total_characters': total_chars,
            'total_rules_detected': total_rules,
            'rules_by_type': rules_by_type,
            'rules_by_category': self._categorize_rules(analysis_results),
            'accuracy_score': self._calculate_accuracy(total_rules, total_chars),
            'rule_density': total_rules / total_chars if total_chars > 0 else 0
        }
    
    def _categorize_rules(self, analysis_results: List[Dict]) -> Dict[str, int]:
        """CatÃ©gorisation amÃ©liorÃ©e"""
        categories = {
            'nun_tanween': ['izhar_hulqi', 'idgham_ghunnah', 'idgham_bila_ghunnah', 'iqlab', 'ikhfa'],
            'meem_sakinah': ['ikhfa_shafawi', 'idgham_mithlayn', 'izhar_shafawi'],
            'madd': ['madd_tabii', 'madd_muttasil', 'madd_munfasil', 'madd_lazim', 'madd_arid'],
            'lam': ['lam_shamsiyya', 'lam_qamariyya'],
            'quality': ['tafkhim_ra', 'tarqiq_ra', 'qalqala', 'ghunnah'],
            'hamz': ['hamzat_wasl', 'hamzat_qat']
        }
        
        category_counts = {category: 0 for category in categories.keys()}
        category_counts['other'] = 0
        
        for result in analysis_results:
            for rule in result['rules']:
                rule_name = rule['rule']
                found = False
                for category, rules in categories.items():
                    if rule_name in rules:
                        category_counts[category] += 1
                        found = True
                        break
                if not found:
                    category_counts['other'] += 1
        
        return category_counts
    
    def _calculate_accuracy(self, total_rules: int, total_chars: int) -> str:
        """Calcul de prÃ©cision amÃ©liorÃ©"""
        if total_chars == 0:
            return "Non calculable"
        
        rule_density = total_rules / total_chars
        
        if rule_density > 0.6:
            return "Ù…Ù†Ø®ÙØ¶Ø© - Ø§ÙƒØªØ´Ø§Ù Ù…ÙØ±Ø·"
        elif rule_density > 0.4:
            return "Ù…ØªÙˆØ³Ø·Ø© - ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†"
        elif rule_density > 0.15:
            return "Ø¬ÙŠØ¯Ø© - Ù…Ù‚Ø¨ÙˆÙ„Ø©"
        else:
            return "Ø¹Ø§Ù„ÙŠØ© - Ù…Ù…ØªØ§Ø²Ø©"
    
    def _generate_summary(self, analysis_results: List[Dict]) -> Dict:
        """RÃ©sumÃ© amÃ©liorÃ©"""
        unique_rules = set()
        total_rules = 0
        
        for result in analysis_results:
            total_rules += result['total_rules']
            for rule in result['rules']:
                unique_rules.add(rule['rule'])
        
        # Niveau de complexitÃ©
        if total_rules > 25:
            complexity = "Ø¹Ø§Ù„ÙŠ Ø¬Ø¯Ø§Ù‹"
        elif total_rules > 15:
            complexity = "Ø¹Ø§Ù„ÙŠ"
        elif total_rules > 8:
            complexity = "Ù…ØªÙˆØ³Ø·"
        elif total_rules > 3:
            complexity = "Ù…Ù†Ø®ÙØ¶"
        else:
            complexity = "Ø¨Ø³ÙŠØ·"
        
        # RÃ¨gles notables
        notable_rules = ['idgham_ghunnah', 'iqlab', 'ikhfa', 'madd_muttasil', 'madd_lazim', 'qalqala']
        found_notable = [rule for rule in unique_rules if rule in notable_rules]
        
        return {
            'total_rules': total_rules,
            'unique_rules': list(unique_rules),
            'complexity_level': complexity,
            'notable_rules': found_notable,
            'unique_rules_count': len(unique_rules)
        }

# Test avec le verset complexe
if __name__ == "__main__":
    analyzer = EnhancedTajweedAnalyzer()
    
    complex_verse = "ÙÙÙ„ÙÙ…ÙÙ‘Ø§ Ø±ÛªØ¡ÛªØ§ Ù‚ÙÙ…ÙÙŠØµÙÙ‡ÙÛ¥ Ù‚ÙØ¯ÙÙ‘ Ù…ÙÙ† Ø¯ÙØ¨ÙØ±Ù– Ù‚ÙØ§Ù„Ù Ø¥ÙÙ†ÙÙ‘Ù‡ÙÛ¥ Ù…ÙÙ† ÙƒÙÙŠÙ’Ø¯ÙÙƒÙÙ†ÙÙ‘ Ø¥ÙÙ†ÙÙ‘ ÙƒÙÙŠÙ’Ø¯ÙÙƒÙÙ†ÙÙ‘ Ø¹ÙØ¸ÙÙŠÙ…ÙÛ–"
    
    print("ğŸ” Analyseur de Tajwid AmÃ©liorÃ©")
    print("=" * 60)
    print(f"Verset: {complex_verse}")
    print("-" * 60)
    
    result = analyzer.analyze_verse(complex_verse)
    
    # Afficher les rÃ¨gles dÃ©tectÃ©es
    rules_found = False
    for analysis in result['analysis']:
        if analysis['rules']:
            rules_found = True
            print(f"\nØ§Ù„Ø­Ø±Ù '{analysis['character']}' (Ø§Ù„Ù…ÙˆØ¶Ø¹ {analysis['position']}):")
            for rule in analysis['rules']:
                print(f"  - {rule['rule']}: {rule['explanation']}")
    
    if not rules_found:
        print("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù‚ÙˆØ§Ø¹Ø¯ ØªØ¬ÙˆÙŠØ¯")
    
    # Statistiques
    stats = result['statistics']
    print(f"\nğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª:")
    print(f"  - Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ø±ÙˆÙ: {stats['total_characters']}")
    print(f"  - Ø¹Ø¯Ø¯ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯: {stats['total_rules_detected']}")
    print(f"  - Ø§Ù„Ø¯Ù‚Ø©: {stats['accuracy_score']}")
    print(f"  - Ø§Ù„ÙƒØ«Ø§ÙØ©: {stats['rule_density']:.3f}")
    print(f"  - ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯: {stats['rules_by_type']}")
    
    summary = result['summary']
    print(f"\nğŸ“ˆ Ø§Ù„Ù…Ù„Ø®Øµ:")
    print(f"  - Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯: {summary['complexity_level']}")
    print(f"  - Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ÙØ±ÙŠØ¯Ø©: {summary['unique_rules']}")
    if summary['notable_rules']:
        print(f"  - Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨Ø§Ø±Ø²Ø©: {summary['notable_rules']}")